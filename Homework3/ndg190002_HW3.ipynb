{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordnet is a lexicial database of English words that provides a semantic hierarchy of words and their relationshipos. It groups words into sets of synonyms, called sysnets, and organizes them based on their semantic relationships, such as hypernymy, hyponymy, meropnymy, and holonymy. WordNet is widely used in nlp and compuational linguistics for tasks such as information retrival, machine translation, and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.book import * \n",
    "import itertools\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baseball', 'baseball_game', 'baseball']\n",
      "Definition:  a ball game played with a bat and ball between two teams of nine players; teams take turns at bat trying to score runs\n",
      "Examples:  ['he played baseball in high school', 'there was a baseball game on every empty lot', 'there was a desire for National League ball in the area', 'play ball!']\n",
      "Lemmas:  ['baseball', 'baseball_game']\n",
      "Synset('ball_game.n.01')\n",
      "Synset('field_game.n.01')\n",
      "Hypernyms:\n",
      "['ball_game', 'ballgame']\n",
      "Hyponyms:\n",
      "['ball', 'five-hitter', '5-hitter', 'four-hitter', '4-hitter', 'hardball', 'no-hit_game', 'no-hitter', 'one-hitter', '1-hitter', 'perfect_game', 'professional_baseball', 'rounders', 'softball', 'softball_game', 'steal', 'stickball', 'stickball_game', 'three-hitter', '3-hitter', 'two-hitter', '2-hitter']\n",
      "Meronyms:\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "Holonyms:\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "Antonyms:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "noun = input(\"Choose a noun: \")\n",
    "noun_synsets = wn.synsets(noun, pos=wn.NOUN)\n",
    "noun_lemmas = [lemma.name() for synset in noun_synsets for lemma in synset.lemmas()]\n",
    "print(noun_lemmas)\n",
    "noun_synset =wn.synset(f'{noun}.n.01')\n",
    "word_definition = noun_synset.definition()\n",
    "word_example = noun_synset.examples()\n",
    "\n",
    "word_lemmas = [lemma.name() for lemma in noun_synset.lemmas()]\n",
    "\n",
    "print(\"Definition: \", word_definition)\n",
    "print(\"Examples: \", word_example)\n",
    "print(\"Lemmas: \", word_lemmas)\n",
    "\n",
    "# traversing up the hierarchy\n",
    "hypernym_synsets = noun_synset.hypernyms()\n",
    "for hypernym_synset in hypernym_synsets:\n",
    "    print(hypernym_synset)\n",
    "    hypernym_hypernym_synsets = hypernym_synset.hypernyms()\n",
    "    for hypernym_hypernym_synset in hypernym_hypernym_synsets:\n",
    "        print(hypernym_hypernym_synset)\n",
    "\n",
    "#hypernyms\n",
    "print(\"Hypernyms:\")\n",
    "print([lemma.name() for hypernym in noun_synset.hypernyms() for lemma in hypernym.lemmas()])\n",
    "# Hyponyms\n",
    "print(\"Hyponyms:\")\n",
    "print([lemma.name() for hyponym in noun_synset.hyponyms() for lemma in hyponym.lemmas()])\n",
    "# Meronyms\n",
    "print(\"Meronyms:\")\n",
    "print([lemma.name() for meronym in noun_synset.part_meronyms() for lemma in meronym.lemmas()])\n",
    "print([lemma.name() for meronym in noun_synset.substance_meronyms() for lemma in meronym.lemmas()])\n",
    "print([lemma.name() for meronym in noun_synset.member_holonyms() for lemma in meronym.lemmas()])\n",
    "\n",
    "\n",
    "# Holonyms\n",
    "print(\"Holonyms:\")\n",
    "print([lemma.name() for holonym in noun_synset.part_holonyms() for lemma in holonym.lemmas()])\n",
    "print([lemma.name() for holonym in noun_synset.substance_holonyms() for lemma in holonym.lemmas()])\n",
    "print([lemma.name() for holonym in noun_synset.member_meronyms() for lemma in holonym.lemmas()])\n",
    "\n",
    "\n",
    "# Antonyms\n",
    "print(\"Antonyms:\")\n",
    "print([lemma.antonyms()[0].name() for lemma in noun_synset.lemmas() if lemma.antonyms()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'scat', 'run', 'scarper', 'turn_tail', 'lam', 'run_away', 'hightail_it', 'bunk', 'head_for_the_hills', 'take_to_the_woods', 'escape', 'fly_the_coop', 'break_away', 'run', 'go', 'pass', 'lead', 'extend', 'operate', 'run', 'run', 'go', 'run', 'flow', 'feed', 'course', 'function', 'work', 'operate', 'go', 'run', 'range', 'run', 'campaign', 'run', 'play', 'run', 'run', 'tend', 'be_given', 'lean', 'incline', 'run', 'run', 'run', 'run', 'run', 'prevail', 'persist', 'die_hard', 'run', 'endure', 'run', 'run', 'execute', 'carry', 'run', 'run', 'guide', 'run', 'draw', 'pass', 'run', 'lead', 'run', 'run', 'black_market', 'run', 'run', 'bleed', 'run', 'run', 'run', 'run_for', 'run', 'run', 'consort', 'run', 'run', 'ply', 'run', 'hunt', 'run', 'hunt_down', 'track_down', 'race', 'run', 'move', 'go', 'run', 'melt', 'run', 'melt_down', 'ladder', 'run', 'run', 'unravel']\n",
      "Definition:  (American football) a play in which a player attempts to carry the ball through or past the opposing team\n",
      "Examples:  ['the defensive line braced to stop the run', 'the coach put great emphasis on running']\n",
      "Lemmas:  ['run', 'running', 'running_play', 'running_game']\n",
      "Synset('football_play.n.01')\n",
      "Synset('play.n.03')\n"
     ]
    }
   ],
   "source": [
    "verb = input(\"Enter a verb: \")\n",
    "verb_synsets = wn.synsets(verb, pos=wn.VERB)\n",
    "verb_lemmas = [lemma.name() for synset in verb_synsets for lemma in synset.lemmas()]\n",
    "print(verb_lemmas)\n",
    "verb_synset =wn.synset(f'{verb}.n.01')\n",
    "verb_definition = verb_synset.definition()\n",
    "verb_example = verb_synset.examples()\n",
    "\n",
    "verbial_lemmas = [lemma.name() for lemma in verb_synset.lemmas()]\n",
    "\n",
    "print(\"Definition: \", verb_definition)\n",
    "print(\"Examples: \", verb_example)\n",
    "print(\"Lemmas: \", verbial_lemmas)\n",
    "\n",
    "# traversing up the hierarchy\n",
    "hypernym_synsets = verb_synset.hypernyms()\n",
    "for hypernym_synset in hypernym_synsets:\n",
    "    print(hypernym_synset)\n",
    "    hypernym_hypernym_synsets = hypernym_synset.hypernyms()\n",
    "    for hypernym_hypernym_synset in hypernym_hypernym_synsets:\n",
    "        print(hypernym_hypernym_synset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'drown', 'float', 'swim'}\n"
     ]
    }
   ],
   "source": [
    "word = input(\"Enter in any verb: \")\n",
    "\n",
    "word_forms = set()\n",
    "\n",
    "for synset in wn.synsets(word, pos=wn.VERB):\n",
    "    for lemma in synset.lemmas():\n",
    "        word_forms.add(lemma.name())\n",
    "    for related_form in wn._morphy(lemma.name(), wn.VERB):\n",
    "        word_forms.add(related_form)\n",
    "\n",
    "print(word_forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wu-Palmer Similarity between sprinting and running: 0.8571428571428571\n",
      "Lesk Algorithm overlap between 'run very fast, usually for a short distance' and 'move fast by using one's feet, with one foot off the ground at any given time': 'None'\n"
     ]
    }
   ],
   "source": [
    "a, b = input(\"Enter in two verbs that are similar each followed by a space \").split(' ')\n",
    "\n",
    "verb1 = wn.synsets(a, pos=wn.VERB)\n",
    "verb2 = wn.synsets(b, pos=wn.VERB)\n",
    "\n",
    "similarity = wn.wup_similarity(verb1[0], verb2[0])\n",
    "print(f\"Wu-Palmer Similarity between {a} and {b}: {similarity}\")\n",
    "\n",
    "\n",
    "def1 = verb1[0].definition()\n",
    "def2 = verb2[0].definition()\n",
    "\n",
    "overlap = lesk(def1, def2)\n",
    "print(f\"Lesk Algorithm overlap between '{def1}' and '{def2}': '{overlap}'\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiwordnet is a resource from NLTK that applies sentiment scores onto the synsets generated by WordNet. It classifies each synset with a positive, negative, and a neutral score, which allows for a more in-depth analysis of the general sentiment by a given text. These scores can be useful because we can apply the emotional tone of a given text into context which can help with larger scale tasks such as monitoring the sentiment of the general public. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love.n.05 0.0 0.0 1.0\n",
      "love.n.01 0.625 0.0 0.375\n",
      "love.n.02 0.375 0.0 0.625\n",
      "love.v.01 0.5 0.0 0.5\n",
      "love.n.04 0.25 0.0 0.75\n",
      "sexual_love.n.02 0.0 0.0 1.0\n",
      "love.v.03 0.625 0.0 0.375\n",
      "love.v.02 1.0 0.0 0.0\n",
      "sleep_together.v.01 0.375 0.125 0.5\n",
      "beloved.n.01 0.125 0.0 0.875\n",
      "I 0.125 0.0 0.875\n",
      "I 0.125 0.0 0.875\n",
      "I 0.125 0.0 0.875\n",
      "I 0.125 0.0 0.875\n",
      "love 0.125 0.0 0.875\n",
      "love 0.125 0.0 0.875\n",
      "love 0.125 0.0 0.875\n",
      "love 0.125 0.0 0.875\n",
      "love 0.125 0.0 0.875\n",
      "love 0.125 0.0 0.875\n",
      "love 0.125 0.0 0.875\n",
      "love 0.125 0.0 0.875\n",
      "love 0.125 0.0 0.875\n",
      "love 0.125 0.0 0.875\n",
      "spending 0.125 0.0 0.875\n",
      "spending 0.125 0.0 0.875\n",
      "spending 0.125 0.0 0.875\n",
      "spending 0.125 0.0 0.875\n",
      "spending 0.125 0.0 0.875\n",
      "time 0.125 0.0 0.875\n",
      "time 0.125 0.0 0.875\n",
      "time 0.125 0.0 0.875\n",
      "time 0.125 0.0 0.875\n",
      "time 0.125 0.0 0.875\n",
      "time 0.125 0.0 0.875\n",
      "time 0.125 0.0 0.875\n",
      "time 0.125 0.0 0.875\n",
      "time 0.125 0.0 0.875\n",
      "time 0.125 0.0 0.875\n",
      "time 0.125 0.0 0.875\n",
      "time 0.125 0.0 0.875\n",
      "time 0.125 0.0 0.875\n",
      "time 0.125 0.0 0.875\n",
      "time 0.125 0.0 0.875\n",
      "family 0.125 0.0 0.875\n",
      "family 0.125 0.0 0.875\n",
      "family 0.125 0.0 0.875\n",
      "family 0.125 0.0 0.875\n",
      "family 0.125 0.0 0.875\n",
      "family 0.125 0.0 0.875\n",
      "family 0.125 0.0 0.875\n",
      "family 0.125 0.0 0.875\n"
     ]
    }
   ],
   "source": [
    "emotion = input(\"Enter in an emotion: \")\n",
    "emotional_sentence = input(\"Write a sentence using the previously entered emotion: \")\n",
    "\n",
    "emo_synsets = set(swn.senti_synsets(emotion))\n",
    "for synset in emo_synsets:\n",
    "    print(synset.synset.name(), synset.pos_score(), synset.neg_score(), synset.obj_score())\n",
    "\n",
    "for word in emotional_sentence.split():\n",
    "    word_synsets = set(swn.senti_synsets(word))\n",
    "    for synsets in word_synsets:\n",
    "        print(word, synset.pos_score(), synset.neg_score(), synset.obj_score())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocation is a set or sequence of words that might often appear together or be interchangeable within a specific language environment, such as English. These words tend to occur more frequently together, usually higher than the expected average chance. Being able to identify the different types of words that appear together is important because it can be used in NLP to tackle tasks like text classification and information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States; fellow citizens; years ago; four years; Federal\n",
      "Government; General Government; American people; Vice President; God\n",
      "bless; Chief Justice; one another; fellow Americans; Old World;\n",
      "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
      "tribes; public debt; foreign nations\n",
      "('Indian', 'tribes'): PMI = 14.22\n",
      "('Western', 'Hemisphere'): PMI = 13.96\n",
      "('ยก', 'Xand'): PMI = 13.05\n",
      "('coordinate', 'branches'): PMI = 12.93\n",
      "('Old', 'World'): PMI = 12.91\n",
      "('George', 'Washington'): PMI = 12.58\n",
      "('faithfully', 'executed'): PMI = 12.54\n",
      "('nuclear', 'weapons'): PMI = 12.41\n",
      "('Chief', 'Magistrate'): PMI = 12.28\n",
      "('middle', 'class'): PMI = 11.98\n"
     ]
    }
   ],
   "source": [
    "text4.collocations()\n",
    "\n",
    "bigram_finder = BigramCollocationFinder.from_words(text4)\n",
    "bigram_finder.apply_freq_filter(5)\n",
    "\n",
    "measures = BigramAssocMeasures()\n",
    "collocations = bigram_finder.score_ngrams(measures.pmi)\n",
    "\n",
    "# sort the collocations by PMI score in descending order and take top 10\n",
    "collocations = sorted(collocations, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "for collocation in collocations:\n",
    "    pmi = collocation[1]\n",
    "    print(f\"{collocation[0]}: PMI = {pmi:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
