# CS4395-Portfolio

This repository will be used to track all of the work done in CS4395 HLT with Dr. Mazidi for the 2023 Spring Semester.

### NLP overview

This pdf is an overview on NLP which talks about some of the basic principles and history behind natural language processing. Also included is current approaches to NLP.

### Homework 1

This was a homework to help us understand the basics of python text processing in preparation for the upcoming assignments. We were to take in employee information formatted in different ways and mutate the data into the same format by using regex to compare strings to ensure that each item was formatted in the right way.

### Homework 2

This was an assignment to help us understand the basic functionality of the NLTK library and preprocessing text, to help better understand how to change data based on requirements. We were to take in a textbook file and return the top 50 most common nouns, and then play a hangman style guessing game by randomly selecting one of the nouns.

### Homework 3

In this assignment we were required to use NLTK's WordNet to understand how it works and learn the necessary skills for future assignments using it. The main points was applying POS tags to words and identifying relative words and definitions that come from synsets of the word. We were then asked to delve into SentiWordNet which involves applying sentiment to a word or phrase. This is the first step into greater sentiment analysis for larger sets of data. We also learned what a collocation is and how to identifiy them and calculate the mutual association each set of collocations in a given text.

### Homework 4

In this assignment, we were tasked to set up and learn how the n-gram modeling technqiue functions and how it can be applied to understand data. The main usage of N-gram modeling is to predict the next word in a given phrase or sentence. This is done by calculating probabilities and determining the word with the highest probable chance of being the next. We were tasked to take in 3 different sets of data, all in a different langauge, convert them into unigram and bigram dictionaries, pickle the dictionaries, and then compute the probability for each language using Laplace smoothing.
